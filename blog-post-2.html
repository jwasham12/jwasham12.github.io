<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Jason Washam Portfolio</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Sunshine - Responsive vCard Template" />
    <meta name="keywords" content="vcard, resposnive, retina, resume, jquery, css3, bootstrap, Sunshine, portfolio" />
    <meta name="author" content="lmtheme" />
    <link rel="shortcut icon" href="favicon.ico">

    <link rel="stylesheet" href="css/bootstrap.css">
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/transition-animations.css">
    <link rel="stylesheet" href="css/owl.carousel.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <link rel="stylesheet" href="css/animate.css">
    <link rel="stylesheet" href="css/main.css" type="text/css">

    <script src="js/jquery-2.1.3.min.js"></script>
    <script src="js/modernizr.custom.js"></script>
  </head>

  <body>
    <!-- Loading animation -->
    <div class="preloader">
      <div class="preloader-animation">
        <div class="dot1"></div>
        <div class="dot2"></div>
      </div>
    </div>
    <!-- /Loading animation -->

    <!-- Single Post page -->
    <div id="page" class="page blog-post-page">
      <!-- Header -->
      <header id="site_header" class="header">
        <div class="my-photo">
          <img src="images/bio.jpg" alt="image">
          <div class="mask"></div>
        </div>

        <div class="site-title-block">
          <h1 class="site-title">Jason Washam</h1>
          <p class="site-description">Data Scienctist</p>
        </div>
      </header>
      <!-- /Header -->

      <div id="main" class="site-main">
        <div class="page-wrapper">
          <a class="blog-back-button" href="index.html#blog"><i class="fa fa-angle-left"></i></a>

          <div class="blog-post-main-image">
            <img class="post-image img-responsive" src="images/blog/blog2/rep-dem.jpg" alt="blog-post-2" />
          </div>

          <div class="blog-post-content">
            <h1>Classifying Republican and Democrat Subreddits</h1>
            <ul class="tags">
                <li><a>Reddit</a></li>
                <li><a>Python</a></li>
                <li><a>Application Programming Interface (API)</a></li>
                <li><a>Classification</a></li>
                <li><a>Ensemble Methods</a></li>
                <li><a>Natural Language Process (NLP)</a></li>
            </ul>
            <header>
              <h2>Introduction</h2>
            </header>
            <p>Reddit is an American social news aggregation, web content rating, and discussion website founded by Steven Huffman and Alexis Ohanian in 2005. Registered members submit content to the site such as links, text posts, and images which are organized by subject into user-created boards called "subreddits". Since there are many subreddits covering a variety of topics including news, science, movies, music, and image-sharing, creating a system that could classify posts into each different subreddits might be helpful in the future. In order to demonstrate the filtering system, I obtained the data (title and text) from republican and democrat subreddit and created a classifying model using natural language processing combining with machine learning algorithms.</p>
            <br>

            <header>
              <h2>Obtaining the Data</h2>
            </header>
            <img src="images/blog/blog2/api.png" alt="blog-post-2">
            <p>There were two approaches to obtain the data. The first option was to use web scraping method which is a technique employed to extract large amounts of data from the website. The other option was to use Reddit's application programming interface (API) which is a set of tools to request necessary data from the database. I chose the API method because web scraping is slower than API call and difficult to analyze. So, in order to use API, I had to obtain the API keys from <a href="https://www.reddit.com/login?dest=https%3A%2F%2Fwww.reddit.com%2Fprefs%2Fapps%2F">Reddit Developed Application</a>. In addition, I used the Python package called <a href="https://praw.readthedocs.io/en/latest/">PRAW</a> (Python Reddit API Wrapper) built by Bryce Boe to obtain the clean title, text and label for each subreddit posts.</p>
            <br>

            <header>
              <h2>Missing Data</h2>
            </header>
            <p style="text-align:center;"><img src="images/blog/blog2/missing.png" alt="blog-post-2" width="600" height="400"></p>
            <p>I was able to get 980 posts from the Republican subreddit and 989 posts from the Democrat subreddit. However, there were many posts with missing text. Only 9.6% of republican posts had text and 7.3% of democrat posts with text. Without text, there is nothing to analyze and use natural language process. So, one of the solutions was to make a new variable which combines the title and the text.</p>
            <br>

            <header>
              <h2>Data Cleaning</h2>
            </header>
            <p style="text-align:center;"><img src="images/blog/blog2/clean.png" alt="blog-post-2" width="600" height="400"></p>
            <p>Even though the use of PRAW package helped to obtain clean data, there were some problems to clean before using natural language process. The problem in the title or text was the URL links and numbers. Since the model should analyze words, the title and text should not have any numbers, URL links, and punctuation. So, I used regex to remove all of the unnecessary items. In addition, the labels are changed to numbers (Democrat: 0, Republican: 1).</p>
            <br>

            <header>
              <h2>Natural Language Process</h2>
            </header>
            <img src="images/blog/blog2/nlp.png" alt="blog-post-2">
            <p>Natural Language Process is a method to convert words into numbers in order to process and analyze large amounts of natural language data. I used Countvectorizer and TF-IDF methods which are simple and basic methods in natural language process. Countvectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words. TF-IDF (term frequency-inverse document frequency) provides statistics that is intended to reflect how important a word is to a document in a collection. The graph above shows the most popular words for both methods.</p>
            <br>

            <header>
              <h2>Selecting Model</h2>
            </header>
            <img src="images/blog/blog2/model.png" alt="blog-post-2">
            <p>The three machine learning methods I chose were Logistic Regression, Naive Bayes and Radom Forest. Logistic regression was the most basic classification method and has interpretability. Naive Bayes is known to work well with the natural language process. Random Forest is one of the top ensemble learning methods that is known to produce high accuracy. The graph shows all three model for both Countvectorizer and TF-IDF performs about the same, but train/test accuracy of the Logistic regression and Random Forest classifier are similar which means that the model generalizes well to unseen data. </p>
            <br>

            <header>
              <h2>Tuning the Models</h2>
            </header>
            <img src="images/blog/blog2/random.png" alt="blog-post-2">
            <p>I decided to pick the Logistic regression and Random Forest classifier models to tune for higher accuracy. For Random Forest with TF-IDF vectorizer, I used grid search on different parameters like max features, ngram range, min df and max depth. The best parameters were
              <ul>
                <li>max feature: 456</li>
                <li>ngram range: (1,2)</li>
                <li>min df: 2</li>
                <li>max depth: 11</li>
              </ul>
            </p>

            <img src="images/blog/blog2/log.png" alt="blog-post-2">
            <p>For Logistic regression with TF-IDF vectorizer, I used grid search on parameters like max features, ngram range, min df and C (inverse of regularization strength). The best parameters were
              <ul>
                <li>max feature: 244</li>
                <li>ngram range: (1,2)</li>
                <li>min df: 3</li>
                <li>C : 1</li>
                <li>penalty : L1</li>
              </ul>
            </p>
            <br>

            <header>
              <h2>Final Model</h2>
            </header>
            <p style="text-align:center;"><img src="images/blog/blog2/final.png" alt="blog-post-2" width="600" height="400"></p>
            <p>The impact of predicting the Republican post as Democrat post (True Negative) or vice versa (False positive) are the same. So, the model with high accuracy regardless of true negative and false positive misclassification is the best model in this case. Therefore, the model that is closest to the top left corner of this plot (best accuracy) is the Logistic regression model.</p>

            <img src="images/blog/blog2/interpret.png" alt="blog-post-2">
            <p>With Logistic regression, I can analyze the coefficients of the model and investigate which words have a positive or negative influence. In this case, positive coefficients help to predict Republican post and negative coefficients help to predict Democrat posts.</p>
            <br>

            <header>
              <h2>Further Research</h2>
            </header>
            <img src="images/blog/blog2/misclass.png" alt="blog-post-2">
            <p>Looking into some misclassified posts, there were some posts that could come from both side of subreddit. Just looking at the example above, it is almost impossible to figure out which side of the subreddit the post is from.</p>

            <img src="images/blog/blog2/useless.png" alt="blog-post-2">
            <p>In addition, there were posts that just did not contain enough information to classify. </p>

            <p>In conclusion, I was able to obtain the data from Reddit and use natural language process in order to build a classification model. The best model was the Logistic regression model that produced train accuracy of 0.767 and test accuracy of 0.748. For more detail analysis and code, Check out my <a href="https://github.com/jwasham12/Subreddit-Classification">Github</a></p>
            <br>

            <div class="post-info">
              <span class="autor"><i class="fa fa-fw fa-user"></i> Jason Washam</span>
              <span class="divider">|</span>
              <span class="date"><i class="fa fa-fw fa-clock-o"></i> 10 December, 2018</span>
              <!-- Share Buttons -->
              <div class="btn-group share-buttons pull-right hidden-xs">
                <a href="https://jwasham94@gmail.com" target="_blank" class="btn"><i class="fa fa-envelope-square"></i> </a>
                <a href="https://github.com/jwasham12"  target="_blank" class="btn"><i class="fa fa-github"></i> </a>
                <a href="https://linkedin.com/in/jason-washam" target="_blank" class="btn"><i class="fa fa-linkedin"></i> </a>
              </div>
              <!-- /Share Buttons -->
            </div>
          </div>

        </div>
      </div>
    </div>
    <!-- /Single Post page -->

    <script src="js/bootstrap.min.js"></script>
    <script src="js/page-transition.js"></script>
    <script src="js/imagesloaded.pkgd.min.js"></script>
    <script src="js/validator.js"></script>
    <script src="js/jquery.shuffle.min.js"></script>
    <script src="js/masonry.pkgd.min.js"></script>
    <script src="js/owl.carousel.min.js"></script>
    <script src="js/jquery.magnific-popup.min.js"></script>
    <script src="js/jquery.hoverdir.js"></script>
    <script src="js/main.js"></script>
  </body>
</html>
